{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f718541a-a0b1-4159-adfb-f59551b19f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final project \n",
    "# Notes\n",
    "#1. most important and lengthy part is data preprocessing\n",
    "#2. not sure why the project asked to vectorize the \"text\" column and NOT the one which was obtained by removing all stop words, all airline names etc.\n",
    "#3. when I tried to vectorize that column - called 'preprocessed', the TF-IDF vectorizer started throwing error\n",
    "#4. Another strange problem that I witnessed was that when I removed punctuations, emojis etc. \n",
    "# After that when I started to use \"preprocess\", the strings started to add backslashes etc. and the counts of top 15 occurences started to distort.\n",
    "#5. To rectify the above problem, I had to change the code such that I stopped using tokenizing etc. \n",
    "#6. Also, to use the Vectorizer there can more than one way a) when we do it for a single text; b) when we do for a column of text taken from a csv file\n",
    "# Imp Learnings\n",
    "#1. Using conditional dataframes\n",
    "#2. removing unnecessary columns by use of \"usecols\"\n",
    "#3. many other of course\n",
    "\n",
    "\n",
    "# All imports\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import docx\n",
    "import re\n",
    "from nltk.util import ngrams as ng\n",
    "from nltk import FreqDist as fd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import docx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from sacremoses import MosesDetokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer as twd\n",
    "import string\n",
    "string.punctuation\n",
    "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f254dd0-43a0-455e-89cb-d1b10ef24a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training using pandas module and select only the sentiment and text columns \n",
    "\n",
    "def df_create(file):\n",
    "    f = pd.read_csv(file, usecols=[\"airline_sentiment\",\"text\"])\n",
    "    f.to_csv(\"final_train_data.csv\", index=False)\n",
    "    \n",
    "df_create(\"Final_Project_Tweets-train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f088a7d-dafd-45b7-a5eb-5c86182c7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove the below\n",
    "# Text contains references with ‘@’ \n",
    "#o Text contains links (http , https ) \n",
    "#o Text contains punctuations \n",
    "#o Text contains Emoticons  \n",
    "\n",
    "def tokenize(content):\n",
    "    return wt(content)\n",
    "\n",
    "def TextAfterRemovingPunctuations(file): \n",
    "    df = pd.read_csv(file)\n",
    "    desc = df['text']\n",
    "    token = []\n",
    "    for d in desc:\n",
    "        tok = tokenize(d)\n",
    "        token.append(tok)\n",
    "    to = []\n",
    "    for t in desc:\n",
    "        t = str(t)\n",
    "        #import string\n",
    "        #string.punctuation\n",
    "        #'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        no_punct = []\n",
    "        '''for i in t:\n",
    "            for j in list(string.punctuation):\n",
    "                i = i.replace(j,'').lower()\n",
    "            no_punct.append(i)\n",
    "        #print(no_punct)'''\n",
    "        #x = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', t)\n",
    "        x = re.sub(r'[.,:;@{}()''\"!?#&]','', t) # need to figure out a way to handle cleaning of [], all others seem to get handled\n",
    "        x = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) # removes https & http\n",
    "        # Now try removing emoticons\n",
    "        emoji_pattern =    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "        #print(x)\n",
    "        text = emoji_pattern.sub(r'', x)# no emoji\n",
    "        #print(text)\n",
    "        to.append(text) # this list has all the tokens with punctuation removed and @ removed & emoticons removed\n",
    "    df['wpehttps'] = to\n",
    "    df.to_csv(file)\n",
    "\n",
    "TextAfterRemovingPunctuations(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "062f2345-a568-4455-9783-7aa568267cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary columns keep only 'airline_sentiment', 'wpehttps', 'text'\n",
    "\n",
    "f = pd.read_csv(\"final_train_data.csv\", usecols=['airline_sentiment', 'wpehttps', 'text'])\n",
    "f.to_csv(\"final_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f87b6671-c4c9-49ed-b718-b52e929221e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\SiyaPransh\\AppData\\Local\\Temp\\ipykernel_14548\\2695242876.py:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "  nu = re.sub(r\"[.,:;@{}#&()''\"\"!?\\[|\\]]\", \"\", nu)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most occuring 15 words from Positive Sentiment tweets\n",
      "[('to', 668), ('the', 658), ('for', 486), ('you', 457), ('I', 417), ('a', 352), (\"'SouthwestAir\", 319), (\"'JetBlue\", 283), ('and', 281), (\"'united\", 274), ('my', 243), ('in', 229), ('on', 220), ('flight', 211), ('thanks', 190)]\n",
      "Most occuring 15 words from Negative Sentiment tweets\n",
      "[('to', 4284), ('the', 2900), ('I', 2446), ('a', 2243), ('for', 1971), ('and', 1951), ('on', 1920), ('my', 1593), ('you', 1557), ('flight', 1554), ('is', 1431), ('in', 1216), (\"'united\", 1163), ('of', 1125), (\"'USAirways\", 1059)]\n",
      "Most occuring 15 words from Neutral Sentiment tweets\n",
      "[('to', 1173), ('I', 795), ('the', 697), ('a', 577), ('you', 497), ('on', 455), ('for', 433), ('SouthwestAir', 381), ('united', 370), ('my', 361), ('JetBlue', 349), ('flight', 339), ('in', 337), ('and', 323), ('is', 311)]\n"
     ]
    }
   ],
   "source": [
    "# List down the most common 15 words for each sentiment. Observe the results \n",
    "\n",
    "def liscommonwordsforeachsentiment(file):\n",
    "    df = pd.read_csv(file, usecols=['airline_sentiment', 'wpehttps', 'text'])\n",
    "    #df = pd.read_csv(file)\n",
    "    #wp = df['removed']\n",
    "    wp = df['wpehttps']\n",
    "    #wp = df['wpehttps']\n",
    "    ai = df['airline_sentiment']\n",
    "    \n",
    "    pos_rev = df[df['airline_sentiment'] == \"positive\"] # conditional dataframes\n",
    "    neg_rev = df[df['airline_sentiment'] == \"negative\"]\n",
    "    neu_rev = df[df['airline_sentiment'] == \"neutral\"]\n",
    "\n",
    "    pos = pos_rev['wpehttps']\n",
    "    po = []\n",
    "    for w in pos:\n",
    "        po.append(w)\n",
    "    po = str(po)\n",
    "    po = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', po) # need to figure out a way to handle cleaning of [], all others seem to get handled\n",
    "    #x = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) # removes https & http\n",
    "    split_it = po.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Positive Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "\n",
    "    neg = neg_rev['wpehttps']\n",
    "    ne = []\n",
    "    for w in neg:\n",
    "        ne.append(w)\n",
    "    ne = str(ne)\n",
    "    ne = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', ne)\n",
    "    #ne = re.sub(\"[.,:;@{}#()''\"\"!?\\[|\\]]\", \"\", ne)\n",
    "    split_it = ne.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Negative Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "\n",
    "    neu = neu_rev['wpehttps']\n",
    "    nu = []\n",
    "    for w in neu:\n",
    "        nu.append(w)\n",
    "    nu = str(nu)\n",
    "    nu = re.sub(r\"[.,:;@{}#&()''\"\"!?\\[|\\]]\", \"\", nu)\n",
    "    #nu = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', nu)\n",
    "    split_it = nu.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Neutral Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "    \n",
    "liscommonwordsforeachsentiment(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d78249c9-e1aa-4cc8-a65c-4466c5fe853b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stopwords from all the tweets. Save changes in a new column and list down most common 15 words. \n",
    "# without tokenizing\n",
    "\n",
    "def RemoveStopWords(sentence):\n",
    "    filtered_sentence = []\n",
    "    for w in sentence:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "        #print(filtered_sentence)\n",
    "    return (filtered_sentence)\n",
    "\n",
    "def tokenize(content):\n",
    "    return wt(content)\n",
    "\n",
    "def rsw(file):\n",
    "    df = pd.read_csv(file, usecols=['airline_sentiment', 'text', 'wpehttps'])\n",
    "    wp = df['wpehttps']\n",
    "    ai = df['airline_sentiment']\n",
    "    content = []\n",
    "    c = []\n",
    "    #print(type(wp))\n",
    "    for w in wp:\n",
    "        #print(type(w))\n",
    "        #print(w)\n",
    "        tt = w\n",
    "        content.append(tt)\n",
    "    sentences = []\n",
    "    for we in content:\n",
    "        sentence = we\n",
    "        for word in stop_words:\n",
    "            token = \" \" + word + \" \"\n",
    "            sentence = sentence.replace(token, \" \")\n",
    "            sentence = sentence.replace(\"  \", \" \")\n",
    "            sentence = sentence.replace(\"https\", \" \")\n",
    "            sentence = sentence.replace(\"https\", \" \")\n",
    "        sentences.append(sentence)\n",
    "    df['rsw'] = sentences\n",
    "    df.to_csv(file)\n",
    "\n",
    "rsw(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04d043ab-66b8-42ef-abc7-173a061881fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary columns keep only 'airline_sentiment', 'wpehttps', 'text'\n",
    "\n",
    "f = pd.read_csv(\"final_train_data.csv\", usecols=['airline_sentiment', 'wpehttps', 'text', 'rsw'])\n",
    "f.to_csv(\"final_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45d90e3f-99b6-4572-8f0b-f84cc0a3aa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most occuring 15 words from Positive Sentiment tweets\n",
      "[('I', 417), (\"'SouthwestAir\", 342), (\"'JetBlue\", 304), (\"'united\", 288), ('flight', 211), (\"'AmericanAir\", 195), ('thanks', 190), ('Thank', 169), (\"'USAirways\", 161), ('Thanks', 150), ('thank', 144), (\"',\", 124), ('great', 111), ('service', 97), (\"you',\", 94)]\n",
      "Most occuring 15 words from Negative Sentiment tweets\n",
      "[('I', 2446), ('flight', 1554), (\"'united\", 1407), (\"'USAirways\", 1270), (\"'AmericanAir\", 1062), ('get', 691), (\"'SouthwestAir\", 655), ('Cancelled', 643), (\"'JetBlue\", 480), ('hours', 448), ('service', 435), ('Flight', 398), ('\"united', 396), ('hold', 394), ('2', 391)]\n",
      "Most occuring 15 words from Neutral Sentiment tweets\n",
      "[('I', 795), ('SouthwestAir', 410), ('united', 391), ('JetBlue', 372), ('flight', 341), ('AmericanAir', 304), ('USAirways', 250), ('get', 171), ('-', 134), ('flights', 125), ('VirginAmerica', 115), ('help', 103), ('please', 98), ('Im', 97), ('need', 96)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\SiyaPransh\\AppData\\Local\\Temp\\ipykernel_28696\\2846197693.py:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "  nu = re.sub(r\"[.,:;@{}#&()''\"\"!?\\[|\\]]\", \"\", nu)\n"
     ]
    }
   ],
   "source": [
    "# List down the most common 15 words for each sentiment. Observe the results \n",
    "\n",
    "def liscommonwordsforeachsentiment(file):\n",
    "    df = pd.read_csv(file, usecols=['airline_sentiment', 'wpehttps', 'text', 'rsw'])\n",
    "    #df = pd.read_csv(file)\n",
    "    #wp = df['removed']\n",
    "    wp = df['rsw']\n",
    "    #wp = df['wpehttps']\n",
    "    ai = df['airline_sentiment']\n",
    "    \n",
    "    pos_rev = df[df['airline_sentiment'] == \"positive\"] # conditional dataframes\n",
    "    neg_rev = df[df['airline_sentiment'] == \"negative\"]\n",
    "    neu_rev = df[df['airline_sentiment'] == \"neutral\"]\n",
    "\n",
    "    pos = pos_rev['rsw']\n",
    "    po = []\n",
    "    for w in pos:\n",
    "        po.append(w)\n",
    "    po = str(po)\n",
    "    po = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', po) # need to figure out a way to handle cleaning of [], all others seem to get handled\n",
    "    #x = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) # removes https & http\n",
    "    split_it = po.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Positive Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "\n",
    "    neg = neg_rev['rsw']\n",
    "    ne = []\n",
    "    for w in neg:\n",
    "        ne.append(w)\n",
    "    ne = str(ne)\n",
    "    ne = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', ne)\n",
    "    #ne = re.sub(\"[.,:;@{}#()''\"\"!?\\[|\\]]\", \"\", ne)\n",
    "    split_it = ne.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Negative Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "\n",
    "    neu = neu_rev['rsw']\n",
    "    nu = []\n",
    "    for w in neu:\n",
    "        nu.append(w)\n",
    "    nu = str(nu)\n",
    "    nu = re.sub(r\"[.,:;@{}#&()''\"\"!?\\[|\\]]\", \"\", nu)\n",
    "    #nu = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', nu)\n",
    "    split_it = nu.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Neutral Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "    \n",
    "liscommonwordsforeachsentiment(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb39cdb2-99b5-4c56-adef-c58976630b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary columns keep only 'airline_sentiment', 'wpehttps', 'text'\n",
    "\n",
    "f = pd.read_csv(\"final_train_data.csv\", usecols=['airline_sentiment', 'wpehttps', 'text', 'rsw'])\n",
    "f.to_csv(\"final_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d02d43a-5ca6-4563-9526-d548cf35ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ('americanair', 'united', 'delta', 'southwestair', 'jetblue', 'virginamerica', 'usairways', 'flight', 'plane' ) from all the tweets. Save changes in a new column and list down most common 15 words. \n",
    "\n",
    "def RemoveStopWords(sentence):\n",
    "    filtered_sentence = []\n",
    "    for w in sentence:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "        #print(filtered_sentence)\n",
    "    return (filtered_sentence)\n",
    "\n",
    "def tokenize(content):\n",
    "    return wt(content)\n",
    "\n",
    "def rsw(file):\n",
    "    df = pd.read_csv(file, usecols=['airline_sentiment', 'text', 'wpehttps','rsw'])\n",
    "    wp = df['wpehttps']\n",
    "    ai = df['airline_sentiment']\n",
    "    content = []\n",
    "    c = []\n",
    "    list_words = ['americanair', 'united', 'delta', 'southwestair', 'jetblue', 'virginamerica', 'usairways', 'flight', 'plane', 'the', 'i']\n",
    "    #print(type(wp))\n",
    "    for w in wp:\n",
    "        #print(type(w))\n",
    "        #print(w)\n",
    "        tt = w\n",
    "        content.append(tt)\n",
    "    sentences = []\n",
    "    for we in content:\n",
    "        sentence = we.lower()\n",
    "        for word in list_words:\n",
    "            word = word.lower()\n",
    "            token = \" \" + word + \" \"\n",
    "            token1 = word + \" \"\n",
    "            token2 = \" \" + word\n",
    "            sentence = sentence.replace(token, \" \")\n",
    "            sentence = sentence.replace(token1, \" \")\n",
    "            sentence = sentence.replace(token2, \" \")\n",
    "            sentence = sentence.replace(\"  \", \" \")\n",
    "        sentences.append(sentence)\n",
    "    df['removed'] = sentences\n",
    "    df.to_csv(file)\n",
    "\n",
    "rsw(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d1ba95f-9c27-4177-ad97-08634c5e1bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most occuring 15 words from Positive Sentiment tweets\n",
      "[(\"'\", 1316), ('the', 689), ('to', 675), ('you', 529), ('for', 492), ('i', 441), ('a', 369), ('thanks', 341), ('thank', 327), ('\"', 325), ('and', 304), ('my', 262), ('in', 235), ('on', 232), ('your', 186)]\n",
      "Most occuring 15 words from Negative Sentiment tweets\n",
      "[('to', 4304), (\"'\", 4231), ('the', 3008), ('i', 2600), ('\"', 2339), ('a', 2296), ('and', 2037), ('for', 1988), ('on', 1971), ('you', 1750), ('my', 1716), ('is', 1506), ('in', 1248), ('of', 1134), ('your', 986)]\n",
      "Most occuring 15 words from Neutral Sentiment tweets\n",
      "[('to', 1182), ('i', 842), ('the', 726), ('a', 588), ('\"', 521), ('you', 518), ('on', 468), ('for', 441), ('my', 386), ('is', 371), ('in', 346), ('and', 328), ('can', 296), ('it', 248), ('of', 245)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\SiyaPransh\\AppData\\Local\\Temp\\ipykernel_28696\\1250503209.py:46: SyntaxWarning: invalid escape sequence '\\['\n",
      "  nu = re.sub(r\"[.,:;@{}#&()''\"\"!?\\[|\\]]\", \"\", nu)\n"
     ]
    }
   ],
   "source": [
    "# List down the most common 15 words for each sentiment. Observe the results \n",
    "\n",
    "def liscommonwordsforeachsentiment(file):\n",
    "    df = pd.read_csv(file, usecols=['airline_sentiment', 'wpehttps', 'text', 'rsw','removed'])\n",
    "    #df = pd.read_csv(file)\n",
    "    #wp = df['removed']\n",
    "    wp = df['removed']\n",
    "    #wp = df['wpehttps']\n",
    "    ai = df['airline_sentiment']\n",
    "    \n",
    "    pos_rev = df[df['airline_sentiment'] == \"positive\"] # conditional dataframes\n",
    "    neg_rev = df[df['airline_sentiment'] == \"negative\"]\n",
    "    neu_rev = df[df['airline_sentiment'] == \"neutral\"]\n",
    "\n",
    "    pos = pos_rev['removed']\n",
    "    po = []\n",
    "    for w in pos:\n",
    "        po.append(w)\n",
    "    po = str(po)\n",
    "    po = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', po) # need to figure out a way to handle cleaning of [], all others seem to get handled\n",
    "    #x = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', x, flags=re.MULTILINE) # removes https & http\n",
    "    split_it = po.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Positive Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "\n",
    "    neg = neg_rev['removed']\n",
    "    ne = []\n",
    "    for w in neg:\n",
    "        ne.append(w)\n",
    "    ne = str(ne)\n",
    "    ne = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', ne)\n",
    "    #ne = re.sub(\"[.,:;@{}#()''\"\"!?\\[|\\]]\", \"\", ne)\n",
    "    split_it = ne.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Negative Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "\n",
    "    neu = neu_rev['removed']\n",
    "    nu = []\n",
    "    for w in neu:\n",
    "        nu.append(w)\n",
    "    nu = str(nu)\n",
    "    nu = re.sub(r\"[.,:;@{}#&()''\"\"!?\\[|\\]]\", \"\", nu)\n",
    "    #nu = re.sub(r'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~','', nu)\n",
    "    split_it = nu.split()\n",
    "    Counters_found = Counter(split_it)\n",
    "    most_occur = Counters_found.most_common(15)\n",
    "    print(\"Most occuring 15 words from Neutral Sentiment tweets\")\n",
    "    print(most_occur)\n",
    "    \n",
    "liscommonwordsforeachsentiment(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a248dc4-e92e-4a97-b612-a423ab32fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary columns keep only 'airline_sentiment', 'wpehttps', 'text', 'rsw','removed'\n",
    "\n",
    "f = pd.read_csv(\"final_train_data.csv\", usecols=['airline_sentiment', 'wpehttps', 'text', 'rsw','removed'])\n",
    "f.to_csv(\"final_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11b1e5cc-323d-415c-b1e4-3e6319791ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Sentiments using Label Encoder \n",
    "\n",
    "def label_encode(file):\n",
    "    df = pd.read_csv(file)\n",
    "    label_encoder = LabelEncoder()\n",
    "    c = df['airline_sentiment']\n",
    "    encoded_data = label_encoder.fit_transform(c)\n",
    "    df['label'] = encoded_data\n",
    "    df.to_csv(file)\n",
    "    \n",
    "label_encode(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c71dd47-5358-4df5-9a72-cd1851a06e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary columns keep only 'airline_sentiment', 'wpehttps', 'text', 'rsw','removed'\n",
    "\n",
    "f = pd.read_csv(\"final_train_data.csv\", usecols=['airline_sentiment', 'wpehttps', 'text', 'rsw','removed', 'label'])\n",
    "f.to_csv(\"final_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "84ec22d7-15ec-41d3-878b-79da8a46c2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the Text Column. \n",
    "\n",
    "def tokenize(content):\n",
    "    return wt(content)\n",
    "\n",
    "def RemoveStopWords(content):\n",
    "    filtered_sentence = []\n",
    "    for w in content:\n",
    "        if w.lower() not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return (filtered_sentence)\n",
    "\n",
    "def Lemmatize(content):\n",
    "        return(wl.lemmatize(content))\n",
    "\n",
    "def refine(user_string):     \n",
    "    #val = input(\"Enter your string: \")     \n",
    "    #print(\"Entered String : \", str(user_string))     \n",
    "    text_tokens = tokenize(user_string)     \n",
    "    #print(\"Entered String (tokenized):\\n\", text_tokens)     \n",
    "    removed_stop_words = text_tokens \n",
    "    #print(\"Entered String (Stop Words Removed):\\n\", removed_stop_words)     \n",
    "    #lemma =  Lemmatize(list(removed_stop_words))     \n",
    "    #print(\"Entered String (Lemmatized):\\n\") \n",
    "    lemmatized_list = []\n",
    "    for word in removed_stop_words:         \n",
    "        w = Lemmatize(word) \n",
    "        w = str(w)\n",
    "        if (w != \"\"):\n",
    "            lemmatized_list.append(w)\n",
    "    return(lemmatized_list) \n",
    "\n",
    "def AddRefinedDescription(file):\n",
    "    df = pd.read_csv(file)\n",
    "    #print(df)\n",
    "    desc = df['text']\n",
    "    #print(desc)\n",
    "    refined_d = []\n",
    "    for d in desc:\n",
    "        refined = refine(d)\n",
    "        refined_d.append(refined)\n",
    "    df['preprocessed'] = refined_d\n",
    "    df.to_csv(\"final_train_data.csv\")\n",
    "    #print(df)\n",
    "\n",
    "def TFIDFVectorization(S1):\n",
    "    tf_vect = TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')\n",
    "    review_list = [S1]\n",
    "    tf_matrix = tf_vect.fit_transform(review_list)\n",
    "    #print(tf_matrix.toarray())\n",
    "    return(tf_matrix.toarray())\n",
    "\n",
    "def CreateTFIDFVectorization(file):\n",
    "    df = pd.read_csv(file)\n",
    "    desc = df['preprocessed']\n",
    "    refined_tokens = []\n",
    "    for d in desc:\n",
    "        #print(d)\n",
    "        refined = TFIDFVectorization(d)\n",
    "        refined_tokens.append(refined)\n",
    "    df['TF-IDF Vectorizer'] = refined_tokens\n",
    "    df.to_csv(file)\n",
    "\n",
    "AddRefinedDescription(\"final_train_data.csv\")\n",
    "CreateTFIDFVectorization(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d338c6a-981d-4a5d-ac89-5f9eb20650b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing unnecessary columns keep only 'airline_sentiment', 'wpehttps', 'text', 'rsw','removed'\n",
    "\n",
    "f = pd.read_csv(\"final_train_data.csv\", usecols=['airline_sentiment', 'wpehttps', 'text', 'rsw','removed', 'label', 'TF-IDF Vectorizer', 'preprocessed'])\n",
    "f.to_csv(\"final_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7ac7450-f741-43f2-87f2-6cad468515e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(10640, 11753)\n",
      "(10640, 11753)\n",
      "    00  000  000114  000lbs  0016  00a  00am  00p  00pm   01  ...  zrh  \\\n",
      "0  0.0  0.0     0.0     0.0   0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0   \n",
      "1  0.0  0.0     0.0     0.0   0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0   \n",
      "2  0.0  0.0     0.0     0.0   0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0   \n",
      "3  0.0  0.0     0.0     0.0   0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0   \n",
      "4  0.0  0.0     0.0     0.0   0.0  0.0   0.0  0.0   0.0  0.0  ...  0.0   \n",
      "\n",
      "   zrh_airport  zsdgzydnde  zsuztnaijq  ztrdwv0n4l  zukes  zurich  zv2pt6trk9  \\\n",
      "0          0.0         0.0         0.0         0.0    0.0     0.0         0.0   \n",
      "1          0.0         0.0         0.0         0.0    0.0     0.0         0.0   \n",
      "2          0.0         0.0         0.0         0.0    0.0     0.0         0.0   \n",
      "3          0.0         0.0         0.0         0.0    0.0     0.0         0.0   \n",
      "4          0.0         0.0         0.0         0.0    0.0     0.0         0.0   \n",
      "\n",
      "   zv6cfpohl5  zzps5ywve2  \n",
      "0         0.0         0.0  \n",
      "1         0.0         0.0  \n",
      "2         0.0         0.0  \n",
      "3         0.0         0.0  \n",
      "4         0.0         0.0  \n",
      "\n",
      "[5 rows x 11753 columns]\n"
     ]
    }
   ],
   "source": [
    "def cm(file):\n",
    "    df = pd.read_csv(file)\n",
    "    desc = df['preprocessed']\n",
    "    y = df['label']\n",
    "    count_vect = TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')\n",
    "    X_counts = count_vect.fit_transform(desc)\n",
    "    #print(type(X_counts))\n",
    "    #print(X_counts.shape)\n",
    "    X_names = count_vect.get_feature_names_out()\n",
    "    X_counts = pd.DataFrame(X_counts.toarray(), columns=X_names)\n",
    "    #print(X_counts.shape)\n",
    "    #print(X_counts.head())\n",
    "    X_counts.to_csv(\"data_final.csv\", sep=',', encoding='utf-8', index=False, header=True)\n",
    "    y.to_csv(\"data_final.csv\")\n",
    "    \n",
    "cm(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "43d185d6-f335-4427-a562-877ad4e269d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[1325    0    0]\n",
      " [ 436    0    0]\n",
      " [ 367    0    0]]\n",
      "Accuracy :\n",
      "0.6226503759398496\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "def run_training(file):\n",
    "    #train, test = train_test_split(df, test_size=0.2)\n",
    "    df = pd.read_csv(file)\n",
    "    dfy = df['label']\n",
    "    dfx = df.drop('label', axis=1)\n",
    "    #print(dfx.shape)\n",
    "    #print(dfy.shape)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(dfx, dfy, test_size=0.2)\n",
    "    #print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "    c = MultinomialNB() \n",
    "    c.fit(X_train, y_train)\n",
    "    y_pred = c.predict(X_test)\n",
    "    #print(type(y_pred))\n",
    "\n",
    "    actualValue = y_test\n",
    "    predictedValue = y_pred\n",
    "    print(\"Confusion Matrix :\")\n",
    "    cmt = confusion_matrix(actualValue, predictedValue)\n",
    "    #tn, fp, fn, tp = confusion_matrix(actualValue, predictedValue).ravel()\n",
    "    print(cmt)\n",
    "    print(\"Accuracy :\")\n",
    "    acc = accuracy_score(actualValue, predictedValue)\n",
    "    print(acc)\n",
    "\n",
    "run_training(\"data_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fccd90c-655a-4115-bdb7-ca74f69300d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
